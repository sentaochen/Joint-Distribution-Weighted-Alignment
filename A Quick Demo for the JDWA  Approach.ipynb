{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "282035b6",
   "metadata": {},
   "source": [
    "#### A Quick Demo for the Joint Distribution Weighted Alignment (JDWA) Approach with a Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de1ebdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchmin import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d6e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the one-hidden-layer neural network model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size=1000, hidden_size=100, output_size=10):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_size, out_features=hidden_size, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=hidden_size, out_features=output_size, bias=True)\n",
    "    def forward(self, X):\n",
    "        FX = F.relu(self.fc1(X)) # hidden layer activation features\n",
    "        prob = F.softmax(self.fc2(FX), dim=1) # probability output\n",
    "        return FX, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45d58ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap the Joint Distribution Weighted Alignment (JDWA) approach as a class, following the sklearn style\n",
    "class JDWA:\n",
    "    \"\"\"\n",
    "    In the training procedure, the total batch size per iteration is: batch_size * num_class * num_domain\n",
    "    For instance, if there are 5 domains with 68 classes in each domain, \n",
    "    then batch_size=4 means drawing 4 samples from every class in every domain,\n",
    "    resulting in 4*68*5 samples in the total batch size.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=1024, hidden_size=512, output_size=68, seed=1000, device=torch.device('cpu'),\n",
    "                 epoch_pretrain=200, epoch=200, lamdaRE=1, lamdaE=0.01, epsilon=1e-3, batch_size=4, lr=1e-3, log=False):\n",
    "        args_values = locals()\n",
    "        args_values.pop(\"self\")\n",
    "        for arg,value in args_values.items():\n",
    "            setattr(self, arg, value)\n",
    " \n",
    "    def fit(self, X_list, y_list, Xt, yt):\n",
    "        class_labels = np.unique(y_list[0])\n",
    "        n, c = len(X_list), len(class_labels) # number of source domains, number of classes\n",
    "        # generate random target labels\n",
    "        yt_rand = np.random.choice(a=class_labels, size=len(Xt), replace=True, p=1.0 * np.ones(c) / c) \n",
    "        X_list.append(Xt), y_list.append(yt_rand)\n",
    "        \n",
    "        # define the neural network instance and the optimizer\n",
    "        torch.manual_seed(self.seed)\n",
    "        net = NeuralNet(input_size=self.input_size, hidden_size=self.hidden_size, output_size=self.output_size).to(self.device)\n",
    "        optimizer = optim.SGD(params=net.parameters(), lr=self.lr, momentum=0.9)\n",
    "\n",
    "        #=====pretrain the network to estimate the pseudo target labels=======\n",
    "        print('Pretraining...')\n",
    "        for epoch in range(self.epoch_pretrain):\n",
    "            dataset_loaders, l = [], 0\n",
    "            for X, y in zip(X_list, y_list):\n",
    "                for i, counts in zip(*np.unique(y, return_counts=True)):\n",
    "                    dataset = np.hstack((X[y==i], y[y==i][:,None], l * np.ones((counts,1))))\n",
    "                    dataset_loaders.append(torch.utils.data.DataLoader(dataset=torch.tensor(dataset),\n",
    "                                                                       batch_size=self.batch_size, shuffle=True, drop_last=False))\n",
    "                l = l + 1 # source domain labels {0, ..., n-1}, target domain label n\n",
    "            \n",
    "            log_loss, m_log_loss, ent_loss, m_ent_loss = 0.0, 0.0, 0.0, 0.0\n",
    "            for batches in zip(*dataset_loaders):\n",
    "                Xyl = torch.cat(batches, dim=0)\n",
    "                X, y, l = Xyl[:,:-2].to(self.device,torch.float32), Xyl[:,-2].to(self.device,torch.int64), Xyl[:,-1].to(self.device,torch.int64)\n",
    "\n",
    "                FX, prob = net(X)\n",
    "                negative_log = 0.0\n",
    "                # weights for the source domains are identical in the pretraining procedure\n",
    "                for i in range(n):\n",
    "                    negative_log += -1.0 / n * torch.mean(torch.sum(torch.log(prob[l==i]) * F.one_hot(y[l==i], c), dim=1))\n",
    "                ent = -torch.mean(torch.sum(prob[l==n] * torch.log(prob[l==n] + 1e-7), dim=1))   # conditional entropy loss\n",
    "                loss = negative_log + self.lamdaE * ent\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                log_loss += negative_log.item() * len(X[l!=n])\n",
    "                m_log_loss += len(X[l!=n])\n",
    "                ent_loss += ent.item() * len(X[l==n])\n",
    "                m_ent_loss += len(X[l==n])\n",
    "\n",
    "            with torch.no_grad():\n",
    "                Xt, yt = torch.as_tensor(Xt, dtype=torch.float32, device=self.device), torch.as_tensor(yt, dtype=torch.int64, device=self.device)    \n",
    "                yt_hat = torch.argmax(net(Xt)[1],dim=1)\n",
    "                correct = torch.sum((yt_hat == yt)).item()\n",
    "                m_test = len(yt)\n",
    "            \n",
    "            if True == self.log:\n",
    "                print('epoch ',epoch, ', log loss ',  \"{:.5f}\".format(log_loss / m_log_loss), \n",
    "                      ', entropy loss ', \"{:.5f}\".format(ent_loss / m_ent_loss), \n",
    "                      ', total loss ', \"{:.5f}\".format(log_loss / m_log_loss + self.lamdaE * ent_loss / m_ent_loss),\n",
    "                      ', test acc. ', \"{:.5f}\".format((correct / m_test) * 100)) \n",
    "        #========================================================\n",
    "        \n",
    "        #=====train the MSDA network====================================\n",
    "        print('Training...')\n",
    "        for epoch in range(self.epoch):\n",
    "            # update the pseudo target labels every epoch\n",
    "            y_list.pop()\n",
    "            y_list.append(yt_hat.cpu().numpy())\n",
    "            dataset_loaders, l = [], 0\n",
    "            for X, y in zip(X_list, y_list):\n",
    "                for i, counts in zip(*np.unique(y, return_counts=True)):\n",
    "                    dataset = np.hstack((X[y==i], y[y==i][:,None], l * np.ones((counts,1))))\n",
    "                    dataset_loaders.append(torch.utils.data.DataLoader(dataset=torch.tensor(dataset),\n",
    "                                                                       batch_size=self.batch_size, shuffle=True, drop_last=False))\n",
    "                l = l + 1 # source domain labels {0, ..., n-1}, target domain label n\n",
    "            \n",
    "            log_loss, m_log_loss, ent_loss, m_ent_loss = 0.0, 0.0, 0.0, 0.0\n",
    "            # n + 1 batches of identical size are drawn from the n + 1 source and target datasets\n",
    "            # each batch contains the same number of samples from each class\n",
    "            for batches in zip(*dataset_loaders):\n",
    "                Xyl = torch.cat(batches, dim=0)\n",
    "                X, y, l = Xyl[:,:-2].to(self.device, torch.float32), Xyl[:,-2].to(self.device, torch.int64), Xyl[:,-1].to(self.device, torch.int64)\n",
    "                \n",
    "                # compute the Gaussian kernel width\n",
    "                pairwise_dist = torch.cdist(X, X, p=2)**2 \n",
    "                sigma = torch.median(pairwise_dist[pairwise_dist!=0]) \n",
    "                \n",
    "                # compute the product kernel matrix\n",
    "                FX, prob = net(X)\n",
    "                FX_norm = torch.sum(FX ** 2, axis = -1)\n",
    "                K = torch.exp(-(FX_norm[:,None] + FX_norm[None,:] - 2 * torch.matmul(FX, FX.t())) / sigma) # feature kernel matrix     \n",
    "                Deltay = torch.as_tensor(y[:,None]==y, dtype=torch.float64, device=FX.device) # label kernel matrix  \n",
    "                P = torch.as_tensor(K * Deltay, dtype=torch.double) # product kernel matrix\n",
    "                \n",
    "                # optimize the relevance weights\n",
    "                def ObjAlpha(alpha):\n",
    "                    alpha = torch.softmax(alpha, dim=0)\n",
    "                    def ObjTheta(theta):\n",
    "                        re = -torch.mean(torch.exp(torch.matmul(P[l==n], theta) - 1.0))\n",
    "                        for i in range(n):\n",
    "                            re += alpha[i] * torch.mean(torch.matmul(P[l==i], theta))\n",
    "                        reg = self.epsilon * torch.matmul(theta, theta) \n",
    "                        return -re + reg\n",
    "                    theta0 = torch.zeros(len(X), dtype=torch.double, device=self.device)\n",
    "                    result = minimize(ObjTheta, theta0, method='l-bfgs', max_iter=5)\n",
    "                    theta = result.x\n",
    "                    negative_log = 0.0\n",
    "                    # the estimated relative entropy as a loss of the relevance weights\n",
    "                    re = -torch.mean(torch.exp(torch.matmul(P[l==n], theta) - 1.0))\n",
    "                    for i in range(n):\n",
    "                        re += alpha[i] * torch.mean(torch.matmul(P[l==i], theta))\n",
    "                        negative_log += -alpha[i] * torch.mean(torch.sum(torch.log(prob[l==i]) * F.one_hot(y[l==i], c), dim=1))           \n",
    "                    return negative_log + self.lamdaRE * re\n",
    "                alpha0 = 1.0 * torch.ones(n, device=self.device) / n \n",
    "                result = minimize(ObjAlpha, alpha0, method='l-bfgs', max_iter=5)\n",
    "                alpha = torch.softmax(result.x, dim=0)\n",
    "                \n",
    "                negative_log = 0.0\n",
    "                for i in range(n):\n",
    "                    negative_log += -alpha[i] * torch.mean(torch.sum(torch.log(prob[l==i]) * F.one_hot(y[l==i], c), dim=1))\n",
    "                def ObjTheta(theta):\n",
    "                    re = -torch.mean(torch.exp(torch.matmul(P[l==n], theta) - 1.0))\n",
    "                    for i in range(n):\n",
    "                        re += alpha[i] * torch.mean(torch.matmul(P[l==i], theta))\n",
    "                    reg = self.epsilon * torch.matmul(theta, theta) \n",
    "                    return -re + reg\n",
    "                theta0 = torch.zeros(len(X), dtype=torch.double, device=self.device)\n",
    "                result = minimize(ObjTheta, theta0, method='l-bfgs')\n",
    "                # the estimated relative entropy as a loss of the feature extractor\n",
    "                re = -torch.mean(torch.exp(torch.matmul(P[l==n], result.x) - 1.0))\n",
    "                for i in range(n):\n",
    "                    re += alpha[i] * torch.mean(torch.matmul(P[l==i], result.x))                \n",
    "                ent = -torch.mean(torch.sum(prob[l==n] * torch.log(prob[l==n] + 1e-7), dim=1))  # conditional entropy loss\n",
    "                loss = negative_log + self.lamdaRE * re + self.lamdaE * ent\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                log_loss += negative_log.item() * len(X[l!=n])\n",
    "                m_log_loss += len(X[l!=n])\n",
    "                ent_loss += ent.item() * len(X[l==n])\n",
    "                m_ent_loss += len(X[l==n])\n",
    "\n",
    "            with torch.no_grad():\n",
    "                Xt, yt = torch.as_tensor(Xt, dtype=torch.float32, device=self.device), torch.as_tensor(yt, dtype=torch.int64, device=self.device)    \n",
    "                yt_hat = torch.argmax(net(Xt)[1],dim=1)\n",
    "                correct = torch.sum((yt_hat == yt)).item()\n",
    "                m_test = len(yt)\n",
    "                \n",
    "            if True == self.log:\n",
    "                print('epoch ',epoch, ', log loss ',  \"{:.5f}\".format(log_loss / m_log_loss), \n",
    "                      ', relative entropy loss ', \"{:.5f}\".format(re.item()), \n",
    "                      ', entropy loss ', \"{:.5f}\".format(ent_loss / m_ent_loss), \n",
    "                      ', total loss ', \"{:.5f}\".format(log_loss / m_log_loss + self.lamdaRE * re.item() + self.lamdaE * ent_loss / m_ent_loss),\n",
    "                      ', test acc. ', \"{:.5f}\".format((correct / m_test) * 100)) \n",
    "            #========================================================     \n",
    "        self.net = net # save the network\n",
    "\n",
    "    def score(self, Xt, yt):\n",
    "        with torch.no_grad():\n",
    "            Xt, yt = torch.as_tensor(Xt, dtype=torch.float32,device=self.device), torch.as_tensor(yt, dtype=torch.int64,device=self.device)    \n",
    "            pred = torch.argmax(self.net(Xt)[1],dim=1)\n",
    "            correct = torch.sum((pred == yt)).item()\n",
    "            m_test = len(yt)\n",
    "        return (correct / m_test) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1901c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import numpy.linalg as la\n",
    "from sklearn.preprocessing import scale,LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ddaa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(tg, domains):\n",
    "    data = sio.loadmat('PIE/' + tg + '.mat')\n",
    "    Xt, yt = data['fea'].astype(np.float64), data['gnd'].ravel()\n",
    "    yt = LabelEncoder().fit(yt).transform(yt).astype(np.float64)\n",
    "    Xt = scale(Xt / Xt.sum(axis=1,keepdims=True))\n",
    "    \n",
    "    Xs_list, ys_list = [], []\n",
    "    for sc in domains:\n",
    "        if sc != tg:\n",
    "            data = sio.loadmat('PIE/' + sc + '.mat')\n",
    "            Xs, ys = data['fea'].astype(np.float64), data['gnd'].ravel()\n",
    "            ys = LabelEncoder().fit(ys).transform(ys).astype(np.float64)\n",
    "            Xs = scale(Xs / Xs.sum(axis=1,keepdims=True))\n",
    "            Xs_list.append(Xs), ys_list.append(ys)        \n",
    "    \n",
    "    return Xs_list, ys_list, Xt, yt\n",
    "\n",
    "domains = ['C05', 'C07', 'C09', 'C27', 'C29']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3c01f84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining...\n",
      "epoch  0 , log loss  4.08850 , entropy loss  4.19485 , total loss  4.09270 , test acc.  16.29652\n",
      "epoch  1 , log loss  3.53632 , entropy loss  4.11505 , total loss  3.54043 , test acc.  31.18247\n",
      "epoch  2 , log loss  2.89301 , entropy loss  3.93695 , total loss  2.89695 , test acc.  47.50900\n",
      "epoch  3 , log loss  2.24238 , entropy loss  3.67903 , total loss  2.24606 , test acc.  59.27371\n",
      "epoch  4 , log loss  1.65814 , entropy loss  3.29787 , total loss  1.66144 , test acc.  65.21609\n",
      "epoch  5 , log loss  1.21132 , entropy loss  2.90704 , total loss  1.21423 , test acc.  69.17767\n",
      "epoch  6 , log loss  0.89307 , entropy loss  2.53652 , total loss  0.89560 , test acc.  70.13806\n",
      "epoch  7 , log loss  0.69678 , entropy loss  2.27251 , total loss  0.69905 , test acc.  72.11885\n",
      "epoch  8 , log loss  0.55812 , entropy loss  2.00926 , total loss  0.56013 , test acc.  73.73950\n",
      "epoch  9 , log loss  0.45823 , entropy loss  1.88834 , total loss  0.46012 , test acc.  74.42977\n",
      "epoch  10 , log loss  0.39204 , entropy loss  1.75389 , total loss  0.39379 , test acc.  74.75990\n",
      "epoch  11 , log loss  0.33319 , entropy loss  1.66131 , total loss  0.33485 , test acc.  75.48019\n",
      "epoch  12 , log loss  0.29190 , entropy loss  1.53209 , total loss  0.29343 , test acc.  75.99040\n",
      "epoch  13 , log loss  0.26026 , entropy loss  1.49476 , total loss  0.26175 , test acc.  76.56062\n",
      "epoch  14 , log loss  0.22680 , entropy loss  1.44279 , total loss  0.22824 , test acc.  76.65066\n",
      "epoch  15 , log loss  0.20687 , entropy loss  1.36401 , total loss  0.20824 , test acc.  77.01080\n",
      "epoch  16 , log loss  0.19341 , entropy loss  1.36064 , total loss  0.19478 , test acc.  77.01080\n",
      "epoch  17 , log loss  0.17135 , entropy loss  1.26913 , total loss  0.17262 , test acc.  77.73109\n",
      "epoch  18 , log loss  0.15497 , entropy loss  1.27716 , total loss  0.15624 , test acc.  77.52101\n",
      "epoch  19 , log loss  0.14702 , entropy loss  1.23089 , total loss  0.14825 , test acc.  78.06122\n",
      "epoch  20 , log loss  0.13760 , entropy loss  1.18433 , total loss  0.13879 , test acc.  78.12125\n",
      "epoch  21 , log loss  0.12798 , entropy loss  1.13732 , total loss  0.12912 , test acc.  78.24130\n",
      "epoch  22 , log loss  0.11466 , entropy loss  1.12739 , total loss  0.11579 , test acc.  78.42137\n",
      "epoch  23 , log loss  0.10811 , entropy loss  1.12220 , total loss  0.10924 , test acc.  78.45138\n",
      "epoch  24 , log loss  0.10253 , entropy loss  1.08145 , total loss  0.10361 , test acc.  78.57143\n",
      "epoch  25 , log loss  0.09694 , entropy loss  1.05621 , total loss  0.09800 , test acc.  78.63145\n",
      "epoch  26 , log loss  0.08619 , entropy loss  1.02639 , total loss  0.08722 , test acc.  78.99160\n",
      "epoch  27 , log loss  0.08588 , entropy loss  1.01066 , total loss  0.08689 , test acc.  78.96158\n",
      "epoch  28 , log loss  0.07924 , entropy loss  1.00561 , total loss  0.08025 , test acc.  78.87155\n",
      "epoch  29 , log loss  0.07430 , entropy loss  0.99058 , total loss  0.07529 , test acc.  79.02161\n",
      "epoch  30 , log loss  0.07321 , entropy loss  0.95945 , total loss  0.07417 , test acc.  79.02161\n",
      "epoch  31 , log loss  0.06677 , entropy loss  0.95756 , total loss  0.06773 , test acc.  79.08163\n",
      "epoch  32 , log loss  0.06672 , entropy loss  0.94644 , total loss  0.06766 , test acc.  78.99160\n",
      "epoch  33 , log loss  0.06108 , entropy loss  0.92751 , total loss  0.06201 , test acc.  78.90156\n",
      "epoch  34 , log loss  0.05888 , entropy loss  0.92097 , total loss  0.05980 , test acc.  79.11164\n",
      "epoch  35 , log loss  0.05417 , entropy loss  0.94002 , total loss  0.05511 , test acc.  79.05162\n",
      "epoch  36 , log loss  0.05300 , entropy loss  0.89865 , total loss  0.05390 , test acc.  79.08163\n",
      "epoch  37 , log loss  0.05131 , entropy loss  0.88790 , total loss  0.05219 , test acc.  78.99160\n",
      "epoch  38 , log loss  0.04771 , entropy loss  0.88926 , total loss  0.04859 , test acc.  79.17167\n",
      "epoch  39 , log loss  0.04495 , entropy loss  0.84967 , total loss  0.04580 , test acc.  79.26170\n",
      "epoch  40 , log loss  0.04425 , entropy loss  0.89648 , total loss  0.04514 , test acc.  79.32173\n",
      "epoch  41 , log loss  0.04137 , entropy loss  0.85405 , total loss  0.04222 , test acc.  79.23169\n",
      "epoch  42 , log loss  0.04086 , entropy loss  0.84219 , total loss  0.04170 , test acc.  79.35174\n",
      "epoch  43 , log loss  0.03811 , entropy loss  0.82584 , total loss  0.03893 , test acc.  79.44178\n",
      "epoch  44 , log loss  0.03836 , entropy loss  0.83032 , total loss  0.03919 , test acc.  79.44178\n",
      "epoch  45 , log loss  0.03713 , entropy loss  0.82908 , total loss  0.03796 , test acc.  79.47179\n",
      "epoch  46 , log loss  0.03572 , entropy loss  0.81609 , total loss  0.03653 , test acc.  79.59184\n",
      "epoch  47 , log loss  0.03457 , entropy loss  0.80976 , total loss  0.03538 , test acc.  79.56182\n",
      "epoch  48 , log loss  0.03284 , entropy loss  0.81324 , total loss  0.03365 , test acc.  79.59184\n",
      "epoch  49 , log loss  0.03151 , entropy loss  0.80710 , total loss  0.03232 , test acc.  79.53181\n",
      "Training...\n",
      "epoch  0 , log loss  0.09418 , relative entropy loss  0.55219 , entropy loss  0.84585 , total loss  5.61692 , test acc.  77.16086\n",
      "epoch  1 , log loss  0.21078 , relative entropy loss  0.31265 , entropy loss  0.96850 , total loss  3.33821 , test acc.  83.73349\n",
      "epoch  2 , log loss  0.14956 , relative entropy loss  0.10345 , entropy loss  1.00651 , total loss  1.18507 , test acc.  86.19448\n",
      "epoch  3 , log loss  0.11986 , relative entropy loss  0.07177 , entropy loss  1.07406 , total loss  0.83865 , test acc.  88.53541\n",
      "epoch  4 , log loss  0.11830 , relative entropy loss  0.04357 , entropy loss  1.10221 , total loss  0.55512 , test acc.  89.25570\n",
      "epoch  5 , log loss  0.11651 , relative entropy loss  0.02960 , entropy loss  1.08340 , total loss  0.41354 , test acc.  90.00600\n",
      "epoch  6 , log loss  0.11294 , relative entropy loss  0.03436 , entropy loss  1.10992 , total loss  0.45770 , test acc.  89.82593\n",
      "epoch  7 , log loss  0.10846 , relative entropy loss  0.02958 , entropy loss  1.07450 , total loss  0.40533 , test acc.  90.42617\n",
      "epoch  8 , log loss  0.10648 , relative entropy loss  0.01776 , entropy loss  1.07306 , total loss  0.28513 , test acc.  90.90636\n",
      "epoch  9 , log loss  0.09996 , relative entropy loss  0.02213 , entropy loss  1.05156 , total loss  0.32230 , test acc.  90.81633\n",
      "epoch  10 , log loss  0.10147 , relative entropy loss  0.02634 , entropy loss  1.02547 , total loss  0.36589 , test acc.  91.20648\n",
      "epoch  11 , log loss  0.09624 , relative entropy loss  0.01687 , entropy loss  1.02602 , total loss  0.26593 , test acc.  91.32653\n",
      "epoch  12 , log loss  0.09328 , relative entropy loss  0.01336 , entropy loss  0.97927 , total loss  0.22785 , test acc.  91.59664\n",
      "epoch  13 , log loss  0.09018 , relative entropy loss  0.01718 , entropy loss  0.94248 , total loss  0.26294 , test acc.  91.59664\n",
      "epoch  14 , log loss  0.08659 , relative entropy loss  0.01639 , entropy loss  0.92234 , total loss  0.25145 , test acc.  91.71669\n",
      "epoch  15 , log loss  0.08460 , relative entropy loss  0.01016 , entropy loss  0.90820 , total loss  0.18708 , test acc.  92.01681\n",
      "epoch  16 , log loss  0.08109 , relative entropy loss  0.01210 , entropy loss  0.89459 , total loss  0.20295 , test acc.  92.25690\n",
      "epoch  17 , log loss  0.08016 , relative entropy loss  0.01317 , entropy loss  0.86596 , total loss  0.21277 , test acc.  92.37695\n",
      "epoch  18 , log loss  0.07867 , relative entropy loss  0.01865 , entropy loss  0.85890 , total loss  0.26600 , test acc.  92.34694\n",
      "epoch  19 , log loss  0.07588 , relative entropy loss  0.00815 , entropy loss  0.83834 , total loss  0.15827 , test acc.  92.43697\n",
      "epoch  20 , log loss  0.07136 , relative entropy loss  0.00644 , entropy loss  0.81288 , total loss  0.13655 , test acc.  92.70708\n",
      "epoch  21 , log loss  0.07146 , relative entropy loss  0.00934 , entropy loss  0.77302 , total loss  0.16567 , test acc.  92.64706\n",
      "epoch  22 , log loss  0.06800 , relative entropy loss  0.01501 , entropy loss  0.77144 , total loss  0.21890 , test acc.  92.76711\n",
      "epoch  23 , log loss  0.06609 , relative entropy loss  0.00492 , entropy loss  0.76391 , total loss  0.11604 , test acc.  92.88715\n",
      "epoch  24 , log loss  0.06604 , relative entropy loss  0.00739 , entropy loss  0.72939 , total loss  0.14070 , test acc.  92.76711\n",
      "epoch  25 , log loss  0.06274 , relative entropy loss  0.00758 , entropy loss  0.72167 , total loss  0.13925 , test acc.  93.06723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  26 , log loss  0.06221 , relative entropy loss  0.00480 , entropy loss  0.71180 , total loss  0.11096 , test acc.  93.09724\n",
      "epoch  27 , log loss  0.05998 , relative entropy loss  0.01640 , entropy loss  0.68386 , total loss  0.22470 , test acc.  93.36735\n",
      "epoch  28 , log loss  0.05893 , relative entropy loss  0.00686 , entropy loss  0.65923 , total loss  0.12821 , test acc.  93.24730\n",
      "epoch  29 , log loss  0.05796 , relative entropy loss  -0.00188 , entropy loss  0.64731 , total loss  0.03982 , test acc.  93.39736\n",
      "epoch  30 , log loss  0.05614 , relative entropy loss  0.00214 , entropy loss  0.64028 , total loss  0.07815 , test acc.  93.48739\n",
      "epoch  31 , log loss  0.05545 , relative entropy loss  0.00569 , entropy loss  0.63339 , total loss  0.11301 , test acc.  93.54742\n",
      "epoch  32 , log loss  0.05403 , relative entropy loss  0.01427 , entropy loss  0.62097 , total loss  0.19735 , test acc.  93.81753\n",
      "epoch  33 , log loss  0.05262 , relative entropy loss  0.00354 , entropy loss  0.60563 , total loss  0.08862 , test acc.  93.90756\n",
      "epoch  34 , log loss  0.05111 , relative entropy loss  0.00158 , entropy loss  0.57085 , total loss  0.06748 , test acc.  93.84754\n",
      "epoch  35 , log loss  0.05078 , relative entropy loss  0.00756 , entropy loss  0.57291 , total loss  0.12693 , test acc.  93.81753\n",
      "epoch  36 , log loss  0.05013 , relative entropy loss  0.00583 , entropy loss  0.57389 , total loss  0.10896 , test acc.  93.81753\n",
      "epoch  37 , log loss  0.04897 , relative entropy loss  0.01089 , entropy loss  0.56687 , total loss  0.15847 , test acc.  94.17767\n",
      "epoch  38 , log loss  0.04730 , relative entropy loss  0.00246 , entropy loss  0.54538 , total loss  0.07245 , test acc.  94.05762\n",
      "epoch  39 , log loss  0.04620 , relative entropy loss  0.00907 , entropy loss  0.51635 , total loss  0.13740 , test acc.  94.23770\n",
      "epoch  40 , log loss  0.04528 , relative entropy loss  -0.00398 , entropy loss  0.52321 , total loss  0.00602 , test acc.  94.23770\n",
      "epoch  41 , log loss  0.04517 , relative entropy loss  0.00128 , entropy loss  0.49000 , total loss  0.05843 , test acc.  94.35774\n",
      "epoch  42 , log loss  0.04482 , relative entropy loss  0.00461 , entropy loss  0.49266 , total loss  0.09143 , test acc.  94.26771\n",
      "epoch  43 , log loss  0.04241 , relative entropy loss  0.00320 , entropy loss  0.47590 , total loss  0.07492 , test acc.  94.23770\n",
      "epoch  44 , log loss  0.04183 , relative entropy loss  0.00396 , entropy loss  0.47383 , total loss  0.08191 , test acc.  94.26771\n",
      "epoch  45 , log loss  0.04255 , relative entropy loss  0.00024 , entropy loss  0.48766 , total loss  0.04548 , test acc.  94.35774\n",
      "epoch  46 , log loss  0.04082 , relative entropy loss  0.00595 , entropy loss  0.46986 , total loss  0.10078 , test acc.  94.32773\n",
      "epoch  47 , log loss  0.04084 , relative entropy loss  -0.00045 , entropy loss  0.45130 , total loss  0.03676 , test acc.  94.29772\n",
      "epoch  48 , log loss  0.04018 , relative entropy loss  -0.00565 , entropy loss  0.42821 , total loss  -0.01589 , test acc.  94.44778\n",
      "epoch  49 , log loss  0.04057 , relative entropy loss  -0.00084 , entropy loss  0.43091 , total loss  0.03263 , test acc.  94.41777\n",
      "epoch  50 , log loss  0.03919 , relative entropy loss  0.00329 , entropy loss  0.44803 , total loss  0.07250 , test acc.  94.35774\n",
      "epoch  51 , log loss  0.03832 , relative entropy loss  -0.00294 , entropy loss  0.42906 , total loss  0.00939 , test acc.  94.38776\n",
      "epoch  52 , log loss  0.03863 , relative entropy loss  0.00164 , entropy loss  0.42348 , total loss  0.05546 , test acc.  94.35774\n",
      "epoch  53 , log loss  0.03738 , relative entropy loss  -0.00201 , entropy loss  0.41140 , total loss  0.01768 , test acc.  94.35774\n",
      "epoch  54 , log loss  0.03613 , relative entropy loss  0.00834 , entropy loss  0.41383 , total loss  0.11994 , test acc.  94.38776\n",
      "epoch  55 , log loss  0.03664 , relative entropy loss  -0.01145 , entropy loss  0.39802 , total loss  -0.07747 , test acc.  94.38776\n",
      "epoch  56 , log loss  0.03557 , relative entropy loss  -0.00381 , entropy loss  0.39861 , total loss  -0.00208 , test acc.  94.41777\n",
      "epoch  57 , log loss  0.03520 , relative entropy loss  0.00163 , entropy loss  0.38509 , total loss  0.05187 , test acc.  94.32773\n",
      "epoch  58 , log loss  0.03539 , relative entropy loss  -0.00329 , entropy loss  0.38858 , total loss  0.00284 , test acc.  94.44778\n",
      "epoch  59 , log loss  0.03455 , relative entropy loss  0.00074 , entropy loss  0.37195 , total loss  0.04235 , test acc.  94.41777\n",
      "epoch  60 , log loss  0.03484 , relative entropy loss  -0.00749 , entropy loss  0.37267 , total loss  -0.03973 , test acc.  94.38776\n",
      "epoch  61 , log loss  0.03380 , relative entropy loss  -0.00248 , entropy loss  0.36716 , total loss  0.00937 , test acc.  94.38776\n",
      "epoch  62 , log loss  0.03358 , relative entropy loss  0.00163 , entropy loss  0.34735 , total loss  0.05019 , test acc.  94.35774\n",
      "epoch  63 , log loss  0.03266 , relative entropy loss  -0.00716 , entropy loss  0.34278 , total loss  -0.03865 , test acc.  94.44778\n",
      "epoch  64 , log loss  0.03195 , relative entropy loss  0.00059 , entropy loss  0.34441 , total loss  0.03817 , test acc.  94.44778\n",
      "epoch  65 , log loss  0.03200 , relative entropy loss  0.00459 , entropy loss  0.34203 , total loss  0.07826 , test acc.  94.44778\n",
      "epoch  66 , log loss  0.03169 , relative entropy loss  -0.00489 , entropy loss  0.33302 , total loss  -0.01689 , test acc.  94.41777\n",
      "epoch  67 , log loss  0.03125 , relative entropy loss  -0.00403 , entropy loss  0.32676 , total loss  -0.00871 , test acc.  94.50780\n",
      "epoch  68 , log loss  0.03080 , relative entropy loss  0.00770 , entropy loss  0.31314 , total loss  0.10810 , test acc.  94.50780\n",
      "epoch  69 , log loss  0.03100 , relative entropy loss  -0.00431 , entropy loss  0.31331 , total loss  -0.01181 , test acc.  94.50780\n",
      "epoch  70 , log loss  0.02974 , relative entropy loss  -0.00517 , entropy loss  0.31256 , total loss  -0.02161 , test acc.  94.50780\n",
      "epoch  71 , log loss  0.02977 , relative entropy loss  0.01080 , entropy loss  0.31145 , total loss  0.13804 , test acc.  94.47779\n",
      "epoch  72 , log loss  0.02929 , relative entropy loss  -0.00678 , entropy loss  0.30556 , total loss  -0.03821 , test acc.  94.47779\n",
      "epoch  73 , log loss  0.02853 , relative entropy loss  -0.00577 , entropy loss  0.30114 , total loss  -0.02883 , test acc.  94.44778\n",
      "epoch  74 , log loss  0.02901 , relative entropy loss  -0.00261 , entropy loss  0.28509 , total loss  0.00315 , test acc.  94.56783\n",
      "epoch  75 , log loss  0.02845 , relative entropy loss  -0.00365 , entropy loss  0.27534 , total loss  -0.00775 , test acc.  94.44778\n",
      "epoch  76 , log loss  0.02794 , relative entropy loss  -0.00284 , entropy loss  0.27641 , total loss  -0.00013 , test acc.  94.44778\n",
      "epoch  77 , log loss  0.02762 , relative entropy loss  -0.00386 , entropy loss  0.28097 , total loss  -0.01074 , test acc.  94.50780\n",
      "epoch  78 , log loss  0.02738 , relative entropy loss  -0.00600 , entropy loss  0.27008 , total loss  -0.03236 , test acc.  94.47779\n",
      "epoch  79 , log loss  0.02717 , relative entropy loss  -0.00434 , entropy loss  0.27104 , total loss  -0.01594 , test acc.  94.47779\n",
      "epoch  80 , log loss  0.02699 , relative entropy loss  0.00610 , entropy loss  0.27526 , total loss  0.08827 , test acc.  94.47779\n",
      "epoch  81 , log loss  0.02732 , relative entropy loss  -0.00563 , entropy loss  0.27161 , total loss  -0.02866 , test acc.  94.50780\n",
      "epoch  82 , log loss  0.02662 , relative entropy loss  -0.00609 , entropy loss  0.25689 , total loss  -0.03399 , test acc.  94.59784\n",
      "epoch  83 , log loss  0.02669 , relative entropy loss  -0.00430 , entropy loss  0.27088 , total loss  -0.01601 , test acc.  94.56783\n",
      "epoch  84 , log loss  0.02653 , relative entropy loss  -0.00846 , entropy loss  0.26113 , total loss  -0.05785 , test acc.  94.53782\n",
      "epoch  85 , log loss  0.02574 , relative entropy loss  0.00546 , entropy loss  0.25294 , total loss  0.08056 , test acc.  94.62785\n",
      "epoch  86 , log loss  0.02551 , relative entropy loss  -0.00667 , entropy loss  0.25183 , total loss  -0.04089 , test acc.  94.68788\n",
      "epoch  87 , log loss  0.02604 , relative entropy loss  -0.00535 , entropy loss  0.25368 , total loss  -0.02724 , test acc.  94.62785\n",
      "epoch  88 , log loss  0.02540 , relative entropy loss  -0.01142 , entropy loss  0.24745 , total loss  -0.08858 , test acc.  94.68788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  89 , log loss  0.02542 , relative entropy loss  -0.01168 , entropy loss  0.24415 , total loss  -0.09113 , test acc.  94.65786\n",
      "epoch  90 , log loss  0.02470 , relative entropy loss  0.00172 , entropy loss  0.23288 , total loss  0.04209 , test acc.  94.59784\n",
      "epoch  91 , log loss  0.02430 , relative entropy loss  0.00039 , entropy loss  0.24212 , total loss  0.02839 , test acc.  94.62785\n",
      "epoch  92 , log loss  0.02442 , relative entropy loss  -0.00093 , entropy loss  0.23770 , total loss  0.01540 , test acc.  94.59784\n",
      "epoch  93 , log loss  0.02402 , relative entropy loss  -0.00751 , entropy loss  0.23811 , total loss  -0.05083 , test acc.  94.65786\n",
      "epoch  94 , log loss  0.02391 , relative entropy loss  0.00050 , entropy loss  0.24041 , total loss  0.02915 , test acc.  94.59784\n",
      "epoch  95 , log loss  0.02385 , relative entropy loss  -0.00542 , entropy loss  0.23334 , total loss  -0.03012 , test acc.  94.65786\n",
      "epoch  96 , log loss  0.02384 , relative entropy loss  -0.01295 , entropy loss  0.21751 , total loss  -0.10547 , test acc.  94.62785\n",
      "epoch  97 , log loss  0.02407 , relative entropy loss  -0.00911 , entropy loss  0.23633 , total loss  -0.06678 , test acc.  94.62785\n",
      "epoch  98 , log loss  0.02346 , relative entropy loss  -0.01202 , entropy loss  0.21329 , total loss  -0.09657 , test acc.  94.62785\n",
      "epoch  99 , log loss  0.02293 , relative entropy loss  -0.01219 , entropy loss  0.21546 , total loss  -0.09873 , test acc.  94.68788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94.68787515006002"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cpu') # 'cuda:0'\n",
    "Xs_list, ys_list, Xt, yt = readData('C05', domains)\n",
    "instance = JDWA(input_size=1024, hidden_size=512, output_size=68, seed=0, device=DEVICE,\n",
    "                         epoch_pretrain=50, epoch=100, lamdaRE=10, lamdaE=1e-3, epsilon=1e-3, batch_size=1, lr=1e-2, log=True)\n",
    "instance.fit(Xs_list, ys_list, Xt, yt)\n",
    "instance.score(Xt, yt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
